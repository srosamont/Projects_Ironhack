{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline \n",
    "sns.set_style(\"whitegrid\")\n",
    "from statsmodels.formula.api import ols\n",
    "from statsmodels.api import OLS\n",
    "from statsmodels.api import add_constant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Assumption tester for OLS model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Assumption_Tester_OLS:\n",
    "    \"\"\"\n",
    "   X - Pandas DataFrame with numerical values. Independent Variable\n",
    "   y - Series with numerical values. Dependent Variable\n",
    "    \n",
    "    Tests a linear regression on the model to see if assumptions are being met\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    \n",
    "    def __init__(self, X,y):\n",
    "        from numpy import ndarray\n",
    "        from pandas import concat\n",
    "        from pandas.core.frame import DataFrame\n",
    "        from pandas.core.series import Series\n",
    "\n",
    "        if type(X) == ndarray:\n",
    "            self.features = ['X'+str(feature+1) for feature in range(X.shape[1])]\n",
    "        elif type(X) == DataFrame:\n",
    "            self.features=X.columns.to_list()\n",
    "        else:\n",
    "            print('Expected numpy array or pandas dataframe as X')\n",
    "            return\n",
    "        if type(y) == ndarray:\n",
    "            self.output = 'y'\n",
    "        elif type(y) == DataFrame:\n",
    "            self.output=y.columns[0]\n",
    "        elif type(y) == Series:\n",
    "            self.output=y.name\n",
    "        else:\n",
    "            print('Expected numpy array or pandas dataframe as X')\n",
    "            return\n",
    "\n",
    "        self.X = X.values if type(X)==DataFrame else X\n",
    "        self.y=y.iloc[:,0].values if type(y)==DataFrame else y.values if type(y)==Series else y\n",
    "        \n",
    "        self.model='not built yet'\n",
    "        self.r2=0\n",
    "        self.results={'Satisfied':[],'Potentially':[],'Violated':[]}\n",
    "    \n",
    "   \n",
    "    def fit_model(self):\n",
    "        from sklearn.linear_model import LinearRegression\n",
    "        \n",
    "        print('Fitting linear regression')        \n",
    "        \n",
    "        #Multi-threading when needed\n",
    "        if self.X.shape[0] > 100000:\n",
    "            self.model = LinearRegression(n_jobs=-1)\n",
    "        else:\n",
    "            self.model = LinearRegression()\n",
    "        self.model.fit(self.X, self.y)\n",
    "        \n",
    "        self.predictions = self.model.predict(self.X)\n",
    "        self.resid = self.y - self.predictions\n",
    "        \n",
    "        \n",
    "    def build_model(self):\n",
    "        self.fit_model()\n",
    "        \n",
    "        # Returning linear regression R^2 and coefficients before performing diagnostics\n",
    "        self.r2 = self.model.score(self.X, self.y)\n",
    "        print()\n",
    "        print('R^2:', self.r2, '\\n')\n",
    "        print('Coefficients')\n",
    "        print('-------------------------------------')\n",
    "        print('Intercept:', self.model.intercept_)\n",
    "        for idx,feature in enumerate(self.model.coef_):\n",
    "            print(f'{self.features[idx]}: {round(feature,2)}')\n",
    "\n",
    "    def linearity(self):\n",
    "        \"\"\"\n",
    "        Linearity: Assumes there is a linear relationship between the predictors and\n",
    "                   the response variable. If not, either a polynomial term or another\n",
    "                   algorithm should be used.\n",
    "        \"\"\"\n",
    "        from pandas import concat\n",
    "        from numpy import arange\n",
    "        from pandas.core.frame import DataFrame\n",
    "        from pandas.core.series import Series        \n",
    "        import seaborn as sns\n",
    "        sns.set()\n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        if type(self.model)==str:\n",
    "            self.fit_model()\n",
    "        \n",
    "        print('\\n=======================================================================================')\n",
    "        print('Assumption 1: Linear Relationship between the Target and the Features')\n",
    "        print('Checking with a scatter plot of actual vs. predicted. Predictions should follow the diagonal line.')\n",
    "        \n",
    "        # Plotting the actual vs predicted values\n",
    "        sns.regplot(self.y,self.predictions, fit_reg=False)\n",
    "        \n",
    "        # Plotting the diagonal line\n",
    "        line_coords = arange(min(self.y.min(),self.predictions.min()), max(self.y.max(),self.predictions.max()))\n",
    "        plt.plot(line_coords, line_coords,  # X and y points\n",
    "                 color='darkorange', linestyle='--')\n",
    "        plt.title('Actual vs. Predicted')\n",
    "        plt.show()\n",
    "        print('If non-linearity is apparent, consider adding a polynomial term \\n\\t\\tor using box-cox transformation to make X or y follow normal distribution')\n",
    "        \n",
    "        print('\\n\\n\\nBuilding a correlation table')\n",
    "        print('\\n=======================================================================================')\n",
    "        df=concat([DataFrame(self.X),Series(self.y)],axis=1)\n",
    "        df.columns=self.features+[self.output]\n",
    "        df_corr=df[df.nunique()[df.nunique()>2].index].corr()[self.output].drop(self.output)\n",
    "        \n",
    "        print(f'\\nParameters that are most likely VIOLATE linearity assumption and their correlation with {self.output}')\n",
    "        display(df_corr[abs(df_corr)<0.25])\n",
    "\n",
    "        print(f'\\nParameters that are most likely FOLLOW linearity assumption and their correlation with {self.output}')\n",
    "        display(df_corr[abs(df_corr)>=0.25])\n",
    "        \n",
    "        \n",
    "        if df_corr[abs(df_corr)<0.25].shape[0]==0:\n",
    "            self.results['Satisfied'].append('Linearity')\n",
    "        elif df_corr[abs(df_corr)>=0.25].shape[0]==0:\n",
    "            self.results['Violated'].append('Linearity')\n",
    "        else:\n",
    "            self.results['Potentially'].append('Linearity')\n",
    "        \n",
    "    def multicollinearity(self):\n",
    "        \"\"\"\n",
    "        Multicollinearity: Assumes that predictors are not correlated with each other. If there is\n",
    "                           correlation among the predictors, then either remove prepdictors with high\n",
    "                           Variance Inflation Factor (VIF) values or perform dimensionality reduction\n",
    "                           This assumption being violated causes issues with interpretability of the \n",
    "                           coefficients and the standard errors of the coefficients.\n",
    "        \"\"\"\n",
    "        from statsmodels.stats.outliers_influence import variance_inflation_factor as VIF\n",
    "        import matplotlib.pyplot as plt\n",
    "        import seaborn as sns\n",
    "        from pandas.core.frame import DataFrame\n",
    "        sns.set()\n",
    "        \n",
    "        if type(self.model)==str:\n",
    "            self.fit_model()\n",
    "            \n",
    "        print('\\n=======================================================================================')\n",
    "        print('Assumption 2: Little to no multicollinearity among predictors')\n",
    "        # Plotting the heatmap\n",
    "        plt.figure(figsize = (10,8))\n",
    "        sns.heatmap(DataFrame(self.X, columns=self.features).corr(), annot=len(self.features)<10, center=0, cmap=sns.diverging_palette(220, 20, as_cmap=True))\n",
    "        plt.title('Correlation of Variables')\n",
    "        plt.show()\n",
    "        print('Variance Inflation Factors (VIF)')\n",
    "        print('> 10: An indication that multicollinearity may be present')\n",
    "        print('> 100: Certain multicollinearity among the variables')\n",
    "        print('-------------------------------------')\n",
    "        # Gathering the VIF for each variable\n",
    "        vifs = {i:VIF(self.X, idx) for idx,i in enumerate(self.features)}\n",
    "        vifs = dict(sorted(vifs.items(), key=lambda x: x[1], reverse=True))\n",
    "        for key, vif in vifs.items():\n",
    "            print(f'{key}: {vif}')\n",
    "        # Gathering and printing total cases of possible or definite multicollinearity\n",
    "        possible_multicollinearity = sum([1 for vif in vifs.values() if vif > 10])\n",
    "        definite_multicollinearity = sum([1 for vif in vifs.values() if vif > 100])\n",
    "        print()\n",
    "        print(f'{possible_multicollinearity} cases of possible multicollinearity')\n",
    "        print(f'{definite_multicollinearity} cases of definite multicollinearity')\n",
    "        print()\n",
    "        if definite_multicollinearity == 0:\n",
    "            if possible_multicollinearity == 0:\n",
    "                print('Assumption satisfied')\n",
    "                self.results['Satisfied'].append('Multicollinearity')\n",
    "            else:\n",
    "                print('Assumption possibly satisfied')\n",
    "                print()\n",
    "                print('Coefficient interpretability may be problematic')\n",
    "                print('Consider removing variables with a high Variance Inflation Factor (VIF)')\n",
    "                self.results['Potentially'].append('Multicollinearity')\n",
    "\n",
    "        else:\n",
    "            print('Assumption not satisfied')\n",
    "            print()\n",
    "            print('Coefficient interpretability will be problematic')\n",
    "            print('Consider removing variables with a high Variance Inflation Factor (VIF)')\n",
    "            self.results['Violated'].append('Multicollinearity')\n",
    "            \n",
    "\n",
    "    \n",
    "    def autocorrelation(self):\n",
    "        \"\"\"\n",
    "        Autocorrelation: Assumes that there is no autocorrelation in the residuals. If there is\n",
    "                         autocorrelation, then there is a pattern that is not explained due to\n",
    "                         the current value being dependent on the previous value.\n",
    "                         This may be resolved by adding a lag variable of either the dependent\n",
    "                         variable or some of the predictors.\n",
    "        \"\"\"\n",
    "        from statsmodels.stats.stattools import durbin_watson        \n",
    "        \n",
    "        if type(self.model)==str:\n",
    "            self.fit_model()\n",
    "        print('\\n=======================================================================================')\n",
    "        print('Assumption 3: No Autocorrelation')\n",
    "        print('\\nPerforming Durbin-Watson Test')\n",
    "        print('Values of 1.5 < d < 2.5 generally show that there is no autocorrelation in the data')\n",
    "        print('0 to 2< is positive autocorrelation')\n",
    "        print('>2 to 4 is negative autocorrelation')\n",
    "        print('-------------------------------------')\n",
    "        durbinWatson = durbin_watson(self.resid)\n",
    "        print('Durbin-Watson:', durbinWatson)\n",
    "        if durbinWatson < 1.5:\n",
    "            print('Signs of positive autocorrelation', '\\n')\n",
    "            print('Assumption not satisfied', '\\n')\n",
    "            self.results['Violated'].append('Autocorrelation')\n",
    "        elif durbinWatson > 2.5:\n",
    "            print('Signs of negative autocorrelation', '\\n')\n",
    "            print('Assumption not satisfied', '\\n')\n",
    "            self.results['Violated'].append('Autocorrelation')\n",
    "        else:\n",
    "            print('Little to no autocorrelation', '\\n')\n",
    "            print('Assumption satisfied')\n",
    "            self.results['Satisfied'].append('Autocorrelation')\n",
    "            \n",
    "\n",
    "    def homoskedasticity(self,p_value_thresh=0.05):\n",
    "        \"\"\"\n",
    "        Homoskedasticity: Assumes that the errors exhibit constant variance\n",
    "        \"\"\"\n",
    "        \n",
    "        from statsmodels.stats.diagnostic import het_breuschpagan\n",
    "        \n",
    "        import matplotlib.pyplot as plt\n",
    "        import seaborn\n",
    "        from numpy import repeat\n",
    "        seaborn.set()\n",
    "        \n",
    "        if type(self.model)==str:\n",
    "            self.fit_model()\n",
    "            \n",
    "        print('\\n=======================================================================================')\n",
    "        print('Assumption 4: Homoskedasticity of Error Terms')\n",
    "        print('Residuals should have relative constant variance')\n",
    "        # Plotting the residuals\n",
    "        plt.subplots(figsize=(12, 6))\n",
    "        ax = plt.subplot(111)  # To remove spines\n",
    "        plt.scatter(x=range(self.X.shape[0]), y=self.resid, alpha=0.5)\n",
    "        plt.plot(repeat(0, self.X.shape[0]), color='darkorange', linestyle='--')\n",
    "        ax.spines['right'].set_visible(False)  # Removing the right spine\n",
    "        ax.spines['top'].set_visible(False)  # Removing the top spine\n",
    "        plt.title('Residuals')\n",
    "        plt.show() \n",
    "        print('If heteroskedasticity is apparent, confidence intervals and predictions will be affected')        \n",
    "        print('\\nConsider removing outliers and preprocessing features - nonlinear transformation can help')\n",
    "        \n",
    "        lnames=['Lagrange Multiplier', 'pvalue for LM','F stats','pvalue for Fstats']\n",
    "        display({lnames[idx]:het_breuschpagan(self.resid,self.X)[idx] for idx in range(4)})\n",
    "        if het_breuschpagan(self.resid,self.X)[3] < p_value_thresh:\n",
    "            print('Signs of positive autocorrelation', '\\n')\n",
    "            print('Assumption potentially not satisfied', '\\n')\n",
    "            self.results['Potentially'].append('Autocorrelation')\n",
    "        else:\n",
    "            print('Signs of negative autocorrelation', '\\n')\n",
    "            print('Assumption satisfied', '\\n')\n",
    "            self.results['Satisfied'].append('Autocorrelation')\n",
    "\n",
    "       \n",
    "        \n",
    "    def normality_resid(self,p_value_thresh=0.05):\n",
    "        \"\"\"\n",
    "        Normality: Assumes that the error terms are normally distributed. If they are not,\n",
    "        nonlinear transformations of variables may solve this.\n",
    "        This assumption being violated primarily causes issues with the confidence intervals\n",
    "        \"\"\"\n",
    "        from statsmodels.stats.diagnostic import normal_ad\n",
    "        from scipy.stats import probplot\n",
    "        import pylab\n",
    "        import matplotlib.pyplot as plt\n",
    "        import seaborn as sns\n",
    "        from numpy import quantile,logical_or\n",
    "        sns.set()\n",
    "\n",
    "        if type(self.model)==str:\n",
    "            self.fit_model()\n",
    "            \n",
    "        print('\\n=======================================================================================')\n",
    "        print('Assumption 5: The error terms are kinda normally distributed')\n",
    "        print()\n",
    "        print('Using the Anderson-Darling test for normal distribution')\n",
    "        # Performing the test on the residuals\n",
    "        p_value = normal_ad(self.resid)[1]\n",
    "        print('p-value from the test - below 0.05 generally means non-normal:', p_value)\n",
    "        # Reporting the normality of the residuals\n",
    "        if p_value < p_value_thresh:\n",
    "            print('Residuals are not normally distributed')\n",
    "        else:\n",
    "            print('Residuals are normally distributed')\n",
    "        # Plotting the residuals distribution\n",
    "        plt.subplots(figsize=(12, 6))\n",
    "        plt.title('Distribution of Residuals')\n",
    "        sns.distplot(self.resid)\n",
    "        plt.show()\n",
    "        print()\n",
    "        if p_value > p_value_thresh:\n",
    "            print('Assumption satisfied')\n",
    "            self.results['Satisfied'].append('Normality')\n",
    "        else:\n",
    "            print('Assumption not satisfied')\n",
    "            self.results['Violated'].append('Normality')\n",
    "            print()\n",
    "            print('Confidence intervals will likely be affected')\n",
    "            print('Try performing nonlinear transformations on variables')\n",
    "    \n",
    "    \n",
    "        print('Building a probability plot')\n",
    "        quantiles=probplot(self.resid, dist='norm', plot=pylab);\n",
    "        plt.show()\n",
    "        qqq=(quantiles[0][1]-quantiles[0][1].mean())/quantiles[0][1].std()-quantiles[0][0]\n",
    "        q75=quantile(qqq,0.75)\n",
    "        q25=quantile(qqq,0.25)\n",
    "\n",
    "        outliers_share=(logical_or(qqq>q75+(q75-q25)*1.7, qqq<q25-(q75-q25)*1.7).sum()/qqq.shape[0]).round(3)\n",
    "        if outliers_share<0.005:\n",
    "            print('Assumption can be considered as satisfied.')\n",
    "            self.results['Satisfied'].append('Sub-Normality')\n",
    "        elif outliers_share<0.05:\n",
    "            self.results['Potentially'].append('Sub-Normality')\n",
    "            print(f'\\nIn your dataset you quite fat tails. You have {outliers_share} potential outliers ({logical_or(qqq>q75+(q75-q25)*1.7, qqq<q25-(q75-q25)*1.7).sum()} rows)')\n",
    "        else:\n",
    "            print(f'\\nIn fact outliers are super significant. Probably it is better to split your dataset into 2 different ones.')\n",
    "            self.results['Violated'].append('Sub-Normality')\n",
    "\n",
    "\n",
    "    def run_all(self):\n",
    "        self.build_model()\n",
    "        self.linearity()\n",
    "        self.multicollinearity()\n",
    "        self.autocorrelation()\n",
    "        self.homoskedasticity()\n",
    "        self.normality_resid()\n",
    "        display(self.results)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Idee :utiliser colonnes libraries avec 1 ou 0 ( livre present ou pas pour faire du hot encoding et utiliser l'info de ces colonnes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataBook.iloc[:,14:73].columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HotEncod(col):\n",
    "    dataBook[col]=dataBook[col].map(lambda x:0 if x==0 else 1)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in dataBook.iloc[:,14:73].columns.to_list():\n",
    "    HotEncod(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummies\n",
    "Dummies_NA=[]\n",
    "Dummies_NA.append('Language')\n",
    "Dummies_NA.append('Categories_stat')\n",
    "Dummies_NA.append('DocType')\n",
    "Dummies_NA.append('Publisher')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=df[['Language','Publisher','Categories_stat','DocType','Nombre_de_localisations',\"Nombre_d'exemplaires\",'Nombre_de_prêts_2017']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=pd.get_dummies(data=df1, columns=Dummies_NA,drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('bookcleanedV1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=df1['Nombre_de_prêts_2017']\n",
    "X=df1.drop('Nombre_de_prêts_2017',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=OLS(y,X)\n",
    "model_fit=model.fit()\n",
    "model_fit.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LS Regression Results\n",
    "Dep. Variable:\tNombre_de_prêts_2017\tR-squared (uncentered):\t0.621\n",
    "Model:\tOLS\tAdj. R-squared (uncentered):\t0.621\n",
    "Method:\tLeast Squares\tF-statistic:\t2.964e+04\n",
    "Date:\tMon, 20 Jul 2020\tProb (F-statistic):\t0.00\n",
    "Time:\t02:18:40\tLog-Likelihood:\t-3.9296e+06\n",
    "No. Observations:\t815531\tAIC:\t7.859e+06\n",
    "Df Residuals:\t815486\tBIC:\t7.860e+06\n",
    "Df Model:\t45\t\t\n",
    "Covariance Type:\tnonrobust\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OLS(y,add_constant(X)).fit().summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P-Hacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(column, X=X,y=y):\n",
    "    if column:\n",
    "        X=X.drop(column, axis=1)\n",
    "    \n",
    "    #buildthemodel\n",
    "    model=OLS(y,add_constant(X))\n",
    "    model_fit=model.fit()\n",
    "    display(model_fit.summary())\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OLS Regression Results\n",
    "Dep. Variable:\tNombre_de_prêts_2017\tR-squared:\t0.599\n",
    "Model:\tOLS\tAdj. R-squared:\t0.599\n",
    "Method:\tLeast Squares\tF-statistic:\t2.708e+04\n",
    "Date:\tMon, 20 Jul 2020\tProb (F-statistic):\t0.00\n",
    "Time:\t02:27:12\tLog-Likelihood:\t-3.9296e+06\n",
    "No. Observations:\t815531\tAIC:\t7.859e+06\n",
    "Df Residuals:\t815485\tBIC:\t7.860e+06\n",
    "Df Model:\t45\t\t\n",
    "Covariance Type:\tnonrobust\t\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=func(\"Publisher_Actes Sud\",X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape,df1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assumptions check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tester=Assumption_Tester_OLS(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tester.run_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.to_csv('X1.csv',index=False)\n",
    "y.to_csv('y1.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=pd.read_csv('X1csv')\n",
    "y=pd.read_csv('y1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.plot.box()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X=X.drop(['Categories_stat_Other','Publisher_Other','DocType_Book'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary=y.describe()\n",
    "summary['IQR']=summary['75%']-summary['25%']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fit=OLS(y,add_constant(X)).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(y_pred-y).describe().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MSE,RMSE, MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean squared error\n",
    "(model_fit.resid**2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Root Mean Squared Error\n",
    "(model_fit.resid**2).mean()**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean Absolute Error\n",
    "model_fit.resid.abs().mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
